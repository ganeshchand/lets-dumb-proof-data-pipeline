// Databricks notebook source
import org.apache.spark.sql.{DataFrame, Dataset, Row}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.streaming.Trigger.{Once, ProcessingTime}

// COMMAND ----------

// MAGIC %md
// MAGIC ## Read

// COMMAND ----------

val input = spark
  .readStream
  .format("rate")
  .option("rowsPerSecond", 1)
  .load()

// COMMAND ----------

// MAGIC %md
// MAGIC ## Transform

// COMMAND ----------

def save(input: Dataset[Row], batchId: Long): Unit = {
  
  val output = input  
    // add audit columns
    .withColumn("etl_insert_timestamp", current_timestamp)
    .withColumn("etl_batch_id", lit(batchId))
    .withColumn("event_date", to_date('timestamp))
   // rename columns
    .withColumnRenamed("timestamp", "event_timestamp")
    .withColumnRenamed("value", "id")
  // finalize output columns
    .selectExpr("etl_batch_id", "etl_insert_timestamp", "event_timestamp", "id", "event_date")
  
  output
   .write
   .format("delta")
   .option("mergeSchema", "true")
   .mode("append")
   .partitionBy("event_date")
   .option("path", "/tmp/deltalake/sample/1/rate")
  .save
  
}

// COMMAND ----------

spark.conf.set("spark.databricks.delta.formatCheck.enabled", "false") // if you make a typo in the config, you won't know. try with 

// COMMAND ----------

// MAGIC %md
// MAGIC ## Write

// COMMAND ----------

val writer = input
  .writeStream
  .queryName("ingest_rateSource_into_delta")
  .option("checkpointLocation", "/tmp/deltalake/sample/1/rate/_checkpoint")
  .trigger(ProcessingTime("30 seconds"))
  .foreachBatch(save _)
  .start()
